{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c983d92",
   "metadata": {},
   "source": [
    "# Задание 2. Свертки и базовые слои \n",
    "Это задание будет являться духовным наследником первого. \n",
    "Вы уже научились делать шаги градиентного спуска и вспомнили, как устроен базовый линейный слой.\n",
    "На этой неделе мы построим прототип базового фреймворка до конца (собственно, многое вы сможете скопировать, если захотите). \n",
    "Хоть вы уже и знаете о torch.nn, для выполнения задания его использовать нельзя. \n",
    "Однако все элементы, которые вы будете реализовывать, достаточно просты.\n",
    "\n",
    "## Задача 1. (2 балла)\n",
    "Реализуйте слой BatchNorm (nn.BatchNorm). \n",
    "## Задача 2. (2 балла)\n",
    "Реализуйте слой Linear (nn.Linear). \n",
    "## Задача 3. (2 балла)\n",
    "Реализуйте слой Dropout(nn.Dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cddc473",
   "metadata": {},
   "source": [
    "Вспомогательные классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10d614ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class Module:\n",
    "    \"\"\"\n",
    "    Базовый класс для всех слоев.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Переключает режим в training (обучение).\"\"\"\n",
    "        self.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"Переключает режим в evaluation (инференс).\"\"\"\n",
    "        self.training = False\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Возвращает список обучаемых параметров (тензоров с requires_grad=True).\"\"\"\n",
    "        return []\n",
    "\n",
    "\n",
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "    Контейнер для последовательного выполнения слоев.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def train(self):\n",
    "        super().train()\n",
    "        for layer in self.layers:\n",
    "            layer.train()\n",
    "\n",
    "    def eval(self):\n",
    "        super().eval()\n",
    "        for layer in self.layers:\n",
    "            layer.eval()\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2476816",
   "metadata": {},
   "source": [
    "Непосредственно слои"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01af00e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Module):\n",
    "    \"\"\"\n",
    "    Слой Batch Normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # Обучаемые параметры\n",
    "        self.gamma = torch.ones(num_features, requires_grad=True)\n",
    "        self.beta = torch.zeros(num_features, requires_grad=True)\n",
    "\n",
    "        # Running stats (не обучаются градиентным спуском)\n",
    "        self.running_mean = torch.zeros(num_features)\n",
    "        self.running_var = torch.ones(num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            # Вычисляем статистики по батчу\n",
    "            mean = x.mean(dim=0)\n",
    "            # Для нормализации используем смещенную оценку дисперсии (как в PyTorch)\n",
    "            var = x.var(dim=0, unbiased=False)\n",
    "\n",
    "            # Обновляем running stats\n",
    "            # Используем несмещенную оценку дисперсии для running_var (как в PyTorch)\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * x.var(dim=0, unbiased=True)\n",
    "\n",
    "            x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        else:\n",
    "            # Используем накопленные статистики\n",
    "            x_hat = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)\n",
    "\n",
    "        return self.gamma * x_hat + self.beta\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    Полносвязный слой (Linear).\n",
    "    y = xW + b\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Инициализация весов (аналогично PyTorch default initialization)\n",
    "        stdv = 1. / math.sqrt(in_features)\n",
    "        self.weights = torch.empty(in_features, out_features, requires_grad=True)\n",
    "        self.weights.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        self.bias = torch.empty(out_features, requires_grad=True)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.bias]\n",
    "\n",
    "\n",
    "class Dropout(Module):\n",
    "    \"\"\"\n",
    "    Слой Dropout.\n",
    "    Во время обучения зануляет элементы входного тензора с вероятностью p.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            # Генерируем маску из распределения Бернулли\n",
    "            # 1 с вероятностью (1-p), 0 с вероятностью p\n",
    "            mask = torch.empty_like(x).bernoulli_(1 - self.p)\n",
    "            # Inverted Dropout: масштабируем, чтобы мат. ожидание не менялось\n",
    "            return x * mask / (1 - self.p)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4f4cf",
   "metadata": {},
   "source": [
    "## Задача 4. {*} (2 балла, 1 за каждый следующий за слой)\n",
    "Реализуйте одно или более из:\n",
    "  - слой ReLU(nn.ReLU)\n",
    "  - слой Sigmoid(nn.Sigmoid)\n",
    "  - слой Softmax(nn.Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d91495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"\n",
    "    Слой активации ReLU.\n",
    "    y = max(0, x)\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.max(x, torch.zeros_like(x))\n",
    "\n",
    "\n",
    "class Sigmoid(Module):\n",
    "    \"\"\"\n",
    "    Слой активации Sigmoid.\n",
    "    y = 1 / (1 + exp(-x))\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "\n",
    "class Softmax(Module):\n",
    "    \"\"\"\n",
    "    Слой активации Softmax.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Стабильный softmax: вычитаем максимум для избежания переполнения экспоненты\n",
    "        x_max = x.max(dim=1, keepdim=True).values\n",
    "        exp_x = torch.exp(x - x_max)\n",
    "        return exp_x / exp_x.sum(dim=1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b4f23",
   "metadata": {},
   "source": [
    "\n",
    "## Задача 5. {*}. \n",
    "Вы получите по 1 дополнительному баллу за слой, \n",
    "если реализуете в рамках фреймворка из задания 3 прошлой работы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de482fd0",
   "metadata": {},
   "source": [
    "Импорт из hw1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a621727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"Базовый класс для оптимизаторов.\"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=0.01):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, grads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"\n",
    "    Adam (Adaptive Moment Estimation).\n",
    "    Идея: Сочетает идеи Momentum и RMSProp. Использует оценки первого (mean) и второго (variance)\n",
    "    моментов градиентов.\n",
    "    m_t = beta1 * m_{t-1} + (1-beta1) * g_t\n",
    "    v_t = beta2 * v_{t-1} + (1-beta2) * g_t^2\n",
    "    Коррекция смещения (bias correction) для начальных шагов.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        super().__init__(params, lr)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = [torch.zeros_like(p) for p in params]\n",
    "        self.v = [torch.zeros_like(p) for p in params]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, grads):\n",
    "        self.t += 1\n",
    "        for i, (param, grad) in enumerate(zip(self.params, grads)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad**2\n",
    "\n",
    "            m_hat = self.m[i] / (1 - self.beta1**self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2**self.t)\n",
    "\n",
    "            param -= self.lr * m_hat / (torch.sqrt(v_hat) + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c638e",
   "metadata": {},
   "source": [
    "Все слои работают с оптимизатором"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b1bfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created.\n",
      "Epoch 0, Loss: 0.6788\n",
      "Epoch 10, Loss: 0.4827\n",
      "Epoch 20, Loss: 0.2554\n",
      "Epoch 30, Loss: 0.1399\n",
      "Epoch 40, Loss: 0.0711\n",
      "Final Loss: 0.0503\n",
      "Test predictions (first 5): tensor([8.7108e-01, 9.7752e-03, 1.0000e+00, 8.8701e-02, 6.7613e-05])\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Генерация синтетических данных\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(100, 10)\n",
    "y = (torch.sum(X, dim=1) > 0).float().unsqueeze(1)  # Простая классификация\n",
    "\n",
    "# Создание модели\n",
    "model = Sequential(\n",
    "\tLinear(10, 20),\n",
    "\tBatchNorm(20),\n",
    "\tReLU(),\n",
    "\tDropout(0.2),\n",
    "\tLinear(20, 1),\n",
    "\tSigmoid()\n",
    ")\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Model created.\")\n",
    "\n",
    "# Обучение\n",
    "model.train()\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\t# Forward\n",
    "\tpredictions = model(X)\n",
    "\n",
    "\t# Loss (Binary Cross Entropy)\n",
    "\tloss = -torch.mean(y * torch.log(predictions + 1e-8) + (1 - y) * torch.log(1 - predictions + 1e-8))\n",
    "\n",
    "\t# Backward\n",
    "\tloss.backward()\n",
    "\n",
    "\t# Optimizer step\n",
    "\tgrads = [p.grad for p in model.parameters()]\n",
    "\twith torch.no_grad():\n",
    "\t\toptimizer.step(grads)\n",
    "\n",
    "\t# Zero gradients\n",
    "\tfor p in model.parameters():\n",
    "\t\tp.grad.zero_()\n",
    "\n",
    "\tif epoch % 10 == 0:\n",
    "\t\tprint(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"Final Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Проверка eval режима\n",
    "model.eval()\n",
    "test_pred = model(X[:5])\n",
    "print(\"Test predictions (first 5):\", test_pred.detach().view(-1))\n",
    "print(\"Test passed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
