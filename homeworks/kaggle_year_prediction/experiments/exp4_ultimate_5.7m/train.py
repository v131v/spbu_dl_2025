"""
Year Prediction MSD - Ultimate Solution
–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å —Å –¥–ª–∏—Ç–µ–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º

–£–ª—É—á—à–µ–Ω–∏—è:
- –û—á–µ–Ω—å –≥–ª—É–±–æ–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: [2048, 1024, 768, 512, 384, 256, 128, 64]
- –ú–Ω–æ–≥–æ —ç–ø–æ—Ö: 1000 —Å –±–æ–ª—å—à–∏–º patience
- –ú–µ–Ω—å—à–∏–π learning rate –¥–ª—è —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
- –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ residual connections
- –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import os
from tqdm import tqdm
import time
import warnings
warnings.filterwarnings('ignore')

def set_seed(seed=42):
    torch.manual_seed(seed)
    np.random.seed(seed)

set_seed(42)

device = torch.device('cpu')
print('='*60)
print('Year Prediction MSD - Ultimate Solution')
print('='*60)
print(f'Device: {device}')
print('Maximum power with extended training')
print('Expected training time: 10-20 minutes on CPU')
print('='*60)


class UltimateYearPredictionNet(nn.Module):
    """
    –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –º–æ—â–Ω–∞—è –≥–ª—É–±–æ–∫–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å
    –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: 90 ‚Üí 2048 ‚Üí 1024 ‚Üí 768 ‚Üí 512 ‚Üí 384 ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 1
    """
    def __init__(self, input_dim=90, hidden_dims=[2048, 1024, 768, 512, 384, 256, 128, 64], dropout_rate=0.4):
        super(UltimateYearPredictionNet, self).__init__()

        # –í—Ö–æ–¥–Ω–æ–π –±–ª–æ–∫ —Å –±–æ–ª–µ–µ —Å–∏–ª—å–Ω–æ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
        self.input_block = nn.Sequential(
            nn.Linear(input_dim, hidden_dims[0]),
            nn.BatchNorm1d(hidden_dims[0]),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )

        # –°–∫—Ä—ã—Ç—ã–µ –±–ª–æ–∫–∏
        self.hidden_blocks = nn.ModuleList()
        for i in range(len(hidden_dims) - 1):
            block = nn.Sequential(
                nn.Linear(hidden_dims[i], hidden_dims[i+1]),
                nn.BatchNorm1d(hidden_dims[i+1]),
                nn.ReLU(),
                nn.Dropout(dropout_rate)
            )
            self.hidden_blocks.append(block)

        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ residual projections –¥–ª—è –ª—É—á—à–µ–≥–æ gradient flow
        self.residual_proj1 = nn.Linear(hidden_dims[0], hidden_dims[2])  # 2048 -> 768
        self.residual_proj2 = nn.Linear(hidden_dims[2], hidden_dims[4])  # 768 -> 384
        self.residual_proj3 = nn.Linear(hidden_dims[4], hidden_dims[6])  # 384 -> 128
        self.residual_proj4 = nn.Linear(hidden_dims[6], hidden_dims[7])  # 128 -> 64

        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        self.output = nn.Linear(hidden_dims[-1], 1)

    def forward(self, x):
        # –í—Ö–æ–¥–Ω–æ–π –±–ª–æ–∫
        out = self.input_block(x)
        identity1 = out

        # –ü–µ—Ä–≤—ã–µ –¥–≤–∞ –±–ª–æ–∫–∞: 2048 -> 1024 -> 768
        out = self.hidden_blocks[0](out)
        out = self.hidden_blocks[1](out)

        # –ü–µ—Ä–≤–æ–µ residual connection
        identity1_proj = self.residual_proj1(identity1)
        out = out + identity1_proj
        identity2 = out

        # –°–ª–µ–¥—É—é—â–∏–µ –±–ª–æ–∫–∏: 768 -> 512 -> 384
        out = self.hidden_blocks[2](out)
        out = self.hidden_blocks[3](out)

        # –í—Ç–æ—Ä–æ–µ residual connection
        identity2_proj = self.residual_proj2(identity2)
        out = out + identity2_proj
        identity3 = out

        # –°–ª–µ–¥—É—é—â–∏–µ –±–ª–æ–∫–∏: 384 -> 256 -> 128
        out = self.hidden_blocks[4](out)
        out = self.hidden_blocks[5](out)

        # –¢—Ä–µ—Ç—å–µ residual connection
        identity3_proj = self.residual_proj3(identity3)
        out = out + identity3_proj
        identity4 = out

        # –ü–æ—Å–ª–µ–¥–Ω–∏–π –±–ª–æ–∫: 128 -> 64
        out = self.hidden_blocks[6](out)

        # –ß–µ—Ç–≤–µ—Ä—Ç–æ–µ residual connection
        identity4_proj = self.residual_proj4(identity4)
        out = out + identity4_proj

        # –í—ã—Ö–æ–¥
        out = self.output(out)
        return out.squeeze()


class EarlyStopping:
    """Early stopping —Å –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º patience"""
    def __init__(self, patience=100, min_delta=0.0001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_score = None
        self.early_stop = False

    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
        elif score > self.best_score - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.counter = 0
        return self.early_stop


def load_and_preprocess_data(data_dir):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print("\nLoading data...")

    train_x = pd.read_csv(os.path.join(data_dir, 'train_x.csv'), index_col=0)
    train_y = pd.read_csv(os.path.join(data_dir, 'train_y.csv'), index_col=0)
    test_x_df = pd.read_csv(os.path.join(data_dir, 'test_x.csv'))

    print(f"Train X shape: {train_x.shape}")
    print(f"Train Y shape: {train_y.shape}")
    print(f"Test X shape: {test_x_df.shape}")

    X_train_full = train_x.values
    y_train_full = train_y['year'].values

    # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ test_x
    test_ids = test_x_df['id'].values
    X_test = test_x_df.drop('id', axis=1).values

    print(f"\nYear range: {y_train_full.min():.0f} - {y_train_full.max():.0f}")
    print(f"Mean year: {y_train_full.mean():.2f}")

    # Train/val split —Å –º–µ–Ω—å—à–∏–º validation set –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full, y_train_full, test_size=0.10, random_state=42
    )

    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_val = scaler.transform(X_val)
    X_test = scaler.transform(X_test)

    print(f"\nTrain set: {X_train.shape[0]} samples")
    print(f"Validation set: {X_val.shape[0]} samples")
    print(f"Test set: {X_test.shape[0]} samples")
    print(f"Test IDs range: {test_ids.min()} - {test_ids.max()}")

    return X_train, X_val, X_test, y_train, y_val, test_ids, scaler


def create_dataloaders(X_train, X_val, y_train, y_val, batch_size=256):
    """–°–æ–∑–¥–∞–Ω–∏–µ DataLoaders —Å –±–æ–ª—å—à–∏–º batch size"""
    train_dataset = TensorDataset(
        torch.FloatTensor(X_train),
        torch.FloatTensor(y_train)
    )
    val_dataset = TensorDataset(
        torch.FloatTensor(X_val),
        torch.FloatTensor(y_val)
    )

    train_loader = DataLoader(train_dataset, batch_size=batch_size,
                             shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=batch_size,
                           shuffle=False, num_workers=0)

    return train_loader, val_loader


def train_epoch(model, train_loader, criterion, optimizer, device, show_progress=True):
    """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ"""
    model.train()
    total_loss = 0

    if show_progress:
        pbar = tqdm(train_loader, desc='Training', leave=False)
    else:
        pbar = train_loader

    for X_batch, y_batch in pbar:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item() * X_batch.size(0)
        if show_progress and isinstance(pbar, tqdm):
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})

    return total_loss / len(train_loader.dataset)


def validate(model, val_loader, criterion, device):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            total_loss += loss.item() * X_batch.size(0)

    return total_loss / len(val_loader.dataset)


def train_model(model, train_loader, val_loader, criterion, optimizer,
                scheduler, num_epochs, device, model_path):
    """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""
    best_val_loss = float('inf')
    early_stopping = EarlyStopping(patience=100, min_delta=0.0001)

    train_losses = []
    val_losses = []

    print("\nStarting training...")
    print(f"Total epochs: {num_epochs}")
    print(f"Early stopping patience: 100")
    print()

    start_time = time.time()

    for epoch in range(num_epochs):
        epoch_start = time.time()

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–µ 20 —ç–ø–æ—Ö –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        show_progress = (epoch % 20 == 0)
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device, show_progress)
        val_loss = validate(model, val_loader, criterion, device)
        scheduler.step(val_loss)

        train_losses.append(train_loss)
        val_losses.append(val_loss)

        epoch_time = time.time() - epoch_start
        elapsed_time = time.time() - start_time

        # –í—ã–≤–æ–¥–∏–º –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö
        if (epoch + 1) % 10 == 0 or epoch == 0:
            current_lr = optimizer.param_groups[0]['lr']
            print(f'Epoch [{epoch+1:4d}/{num_epochs}] '
                  f'Train: {train_loss:.4f} | Val: {val_loss:.4f} | '
                  f'LR: {current_lr:.7f} | Time: {epoch_time:.1f}s | '
                  f'Total: {elapsed_time/60:.1f}m')

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
            }, model_path)
            if (epoch + 1) % 10 == 0 or epoch < 30:
                print(f'  ‚úì Model saved! Best Val Loss: {val_loss:.4f} (RMSE: {np.sqrt(val_loss):.4f})')

        if early_stopping(val_loss):
            print(f'\n‚ö†Ô∏è  Early stopping at epoch {epoch+1}')
            break

    total_time = time.time() - start_time
    print(f'\n{"="*60}')
    print(f'Training completed in {total_time/60:.1f} minutes')
    print(f'Best validation loss: {best_val_loss:.4f}')
    print(f'Best validation RMSE: {np.sqrt(best_val_loss):.4f} years')
    print(f'{"="*60}')

    return train_losses, val_losses


def predict(model, X_test, device, batch_size=256):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    model.eval()
    predictions = []

    X_test_tensor = torch.FloatTensor(X_test)
    test_dataset = TensorDataset(X_test_tensor)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    print("\nGenerating predictions...")
    with torch.no_grad():
        for (X_batch,) in tqdm(test_loader, desc='Predicting'):
            X_batch = X_batch.to(device)
            outputs = model(X_batch)
            predictions.extend(outputs.cpu().numpy())

    return np.array(predictions)


def create_submission(predictions, test_ids, output_path='submission.csv'):
    """–°–æ–∑–¥–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º –¥–ª—è Kaggle

    –§–æ—Ä–º–∞—Ç: index,year
    - index: ID –∏–∑ test_x.csv
    - year: —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ (–æ–∫—Ä—É–≥–ª–µ–Ω–Ω—ã–π –≥–æ–¥)
    """
    # –û–∫—Ä—É–≥–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–æ —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª
    predictions_int = np.round(predictions).astype(int)

    submission = pd.DataFrame({
        'id': test_ids,  # –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è Kaggle
        'year': predictions_int  # –¶–µ–ª—ã–µ —á–∏—Å–ª–∞
    })
    submission.to_csv(output_path, index=False)
    print(f'\n‚úì Submission saved: {output_path}')
    print(f'  Total predictions: {len(predictions)}')
    print(f'  Year range: {predictions_int.min()} - {predictions_int.max()}')
    print(f'  Index range: {test_ids.min()} - {test_ids.max()}')


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""

    data_dir = 'data'

    required_files = ['train_x.csv', 'train_y.csv', 'test_x.csv']
    missing_files = [f for f in required_files if not os.path.exists(os.path.join(data_dir, f))]

    if missing_files:
        print(f"\n‚ùå Error: Missing files: {', '.join(missing_files)}")
        return

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_val, X_test, y_train, y_val, test_ids, scaler = load_and_preprocess_data(data_dir)

    # DataLoaders —Å –±–æ–ª—å—à–∏–º batch size
    batch_size = 256
    train_loader, val_loader = create_dataloaders(
        X_train, X_val, y_train, y_val, batch_size=batch_size
    )

    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
    model = UltimateYearPredictionNet(
        input_dim=90,
        hidden_dims=[2048, 1024, 768, 512, 384, 256, 128, 64],
        dropout_rate=0.4
    ).to(device)

    total_params = sum(p.numel() for p in model.parameters())
    print(f"\nModel: UltimateYearPredictionNet")
    print(f"Total parameters: {total_params:,}")
    print(f"Architecture: 90 ‚Üí 2048 ‚Üí 1024 ‚Üí 768 ‚Üí 512 ‚Üí 384 ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 1")

    # Optimizer –∏ scheduler —Å –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–º LR
    criterion = nn.MSELoss()
    optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=2e-5)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=20
    )

    # –û–±—É—á–µ–Ω–∏–µ (–æ—á–µ–Ω—å –º–Ω–æ–≥–æ —ç–ø–æ—Ö)
    model_path = 'models/best_model_ultimate.pth'
    os.makedirs(os.path.dirname(model_path), exist_ok=True)

    train_losses, val_losses = train_model(
        model, train_loader, val_loader, criterion, optimizer, scheduler,
        num_epochs=1000,  # –û—á–µ–Ω—å –º–Ω–æ–≥–æ —ç–ø–æ—Ö
        device=device,
        model_path=model_path
    )

    # –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    checkpoint = torch.load(model_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"\n‚úì Loaded best model from epoch {checkpoint['epoch']+1}")

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    predictions = predict(model, X_test, device, batch_size=batch_size)

    # Submission
    submission_path = 'submission_ultimate.csv'
    create_submission(predictions, test_ids, submission_path)

    print("\n" + "="*60)
    print("‚úì Training completed successfully!")
    print("="*60)
    print(f"Model saved: {model_path}")
    print(f"Submission: {submission_path}")
    print("\nüéâ Ready to submit to Kaggle!")
    print("="*60)


if __name__ == '__main__':
    main()
